Loading data...
Loading model and tokenizer...
INFO 05-05 15:12:07 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/', speculative_config=None, tokenizer='output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/)
INFO 05-05 15:12:11 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=19028)[0m INFO 05-05 15:12:11 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 05-05 15:12:11 selector.py:27] Using FlashAttention-2 backend.
[36m(RayWorkerWrapper pid=19028)[0m INFO 05-05 15:12:12 selector.py:27] Using FlashAttention-2 backend.
INFO 05-05 15:12:12 pynccl_utils.py:43] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=19028)[0m INFO 05-05 15:12:12 pynccl_utils.py:43] vLLM is using nccl==2.18.1
INFO 05-05 15:12:13 utils.py:132] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=19028)[0m INFO 05-05 15:12:13 utils.py:132] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 05-05 15:12:31 model_runner.py:175] Loading model weights took 7.4829 GB
[36m(RayWorkerWrapper pid=19028)[0m INFO 05-05 15:12:31 model_runner.py:175] Loading model weights took 7.4829 GB
INFO 05-05 15:12:32 distributed_gpu_executor.py:45] # GPU blocks: 61151, # CPU blocks: 4096
INFO 05-05 15:12:33 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 05-05 15:12:33 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=19028)[0m INFO 05-05 15:12:33 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=19028)[0m INFO 05-05 15:12:33 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-05 15:12:40 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
INFO 05-05 15:12:40 model_runner.py:1017] Graph capturing finished in 7 secs.
[36m(RayWorkerWrapper pid=19028)[0m INFO 05-05 15:12:40 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
[36m(RayWorkerWrapper pid=19028)[0m INFO 05-05 15:12:40 model_runner.py:1017] Graph capturing finished in 7 secs.
Calculating accuracy...
Exact match : 0.514783927217589
Loading data...
Loading model and tokenizer...
INFO 05-05 15:14:32 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/', speculative_config=None, tokenizer='output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/)
INFO 05-05 15:14:36 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=21497)[0m INFO 05-05 15:14:36 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 05-05 15:14:37 selector.py:27] Using FlashAttention-2 backend.
[36m(RayWorkerWrapper pid=21497)[0m INFO 05-05 15:14:37 selector.py:27] Using FlashAttention-2 backend.
INFO 05-05 15:14:38 pynccl_utils.py:43] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=21497)[0m INFO 05-05 15:14:38 pynccl_utils.py:43] vLLM is using nccl==2.18.1
INFO 05-05 15:14:38 utils.py:132] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=21497)[0m INFO 05-05 15:14:38 utils.py:132] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 05-05 15:14:56 model_runner.py:175] Loading model weights took 7.4829 GB
[36m(RayWorkerWrapper pid=21497)[0m INFO 05-05 15:14:56 model_runner.py:175] Loading model weights took 7.4829 GB
INFO 05-05 15:14:57 distributed_gpu_executor.py:45] # GPU blocks: 61151, # CPU blocks: 4096
INFO 05-05 15:14:59 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 05-05 15:14:59 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=21497)[0m INFO 05-05 15:14:59 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=21497)[0m INFO 05-05 15:14:59 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-05 15:15:05 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
INFO 05-05 15:15:05 model_runner.py:1017] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=21497)[0m INFO 05-05 15:15:05 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
[36m(RayWorkerWrapper pid=21497)[0m INFO 05-05 15:15:05 model_runner.py:1017] Graph capturing finished in 6 secs.
Calculating accuracy...
Exact match for en: 0.46
Calculating accuracy...
Exact match for es: 0.428
Calculating accuracy...
Exact match for fr: 0.44
Calculating accuracy...
Exact match for de: 0.44
Calculating accuracy...
Exact match for ru: 0.356
Calculating accuracy...
Exact match for zh: 0.38
Calculating accuracy...
Exact match for ja: 0.344
Calculating accuracy...
Exact match for th: 0.364
Calculating accuracy...
Exact match for sw: 0.232
Calculating accuracy...
Exact match for bn: 0.292
Calculating accuracy...
Exact match for te: 0.248
Loading data...
Loading model and tokenizer...
INFO 05-05 15:21:39 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/', speculative_config=None, tokenizer='output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/)
INFO 05-05 15:21:43 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=23982)[0m INFO 05-05 15:21:43 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 05-05 15:21:44 selector.py:27] Using FlashAttention-2 backend.
[36m(RayWorkerWrapper pid=23982)[0m INFO 05-05 15:21:45 selector.py:27] Using FlashAttention-2 backend.
INFO 05-05 15:21:45 pynccl_utils.py:43] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=23982)[0m INFO 05-05 15:21:45 pynccl_utils.py:43] vLLM is using nccl==2.18.1
INFO 05-05 15:21:45 utils.py:132] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=23982)[0m INFO 05-05 15:21:45 utils.py:132] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 05-05 15:22:03 model_runner.py:175] Loading model weights took 7.4829 GB
[36m(RayWorkerWrapper pid=23982)[0m INFO 05-05 15:22:04 model_runner.py:175] Loading model weights took 7.4829 GB
INFO 05-05 15:22:05 distributed_gpu_executor.py:45] # GPU blocks: 61151, # CPU blocks: 4096
[36m(RayWorkerWrapper pid=23982)[0m INFO 05-05 15:22:06 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=23982)[0m INFO 05-05 15:22:06 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-05 15:22:06 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 05-05 15:22:06 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-05 15:22:13 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
INFO 05-05 15:22:13 model_runner.py:1017] Graph capturing finished in 7 secs.
[36m(RayWorkerWrapper pid=23982)[0m INFO 05-05 15:22:13 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
[36m(RayWorkerWrapper pid=23982)[0m INFO 05-05 15:22:13 model_runner.py:1017] Graph capturing finished in 7 secs.
Calculating accuracy...
Exact match for en: 0.504
Calculating accuracy...
Exact match for es: 0.476
Calculating accuracy...
Exact match for fr: 0.408
Calculating accuracy...
Exact match for de: 0.46
Calculating accuracy...
Exact match for ru: 0.452
Calculating accuracy...
Exact match for zh: 0.424
Calculating accuracy...
Exact match for ja: 0.4
Calculating accuracy...
Exact match for th: 0.384
Calculating accuracy...
Exact match for sw: 0.312
Calculating accuracy...
Exact match for bn: 0.368
Calculating accuracy...
Exact match for te: 0.288
Number of examples: 164
INFO 05-05 15:30:18 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/', speculative_config=None, tokenizer='output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=output/self-seq-Meta-Llama-3-8B-flancot_llama_70b/)
INFO 05-05 15:30:22 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=26415)[0m INFO 05-05 15:30:22 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 05-05 15:30:24 selector.py:27] Using FlashAttention-2 backend.
[36m(RayWorkerWrapper pid=26415)[0m INFO 05-05 15:30:24 selector.py:27] Using FlashAttention-2 backend.
INFO 05-05 15:30:24 pynccl_utils.py:43] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=26415)[0m INFO 05-05 15:30:24 pynccl_utils.py:43] vLLM is using nccl==2.18.1
INFO 05-05 15:30:25 utils.py:132] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=26415)[0m INFO 05-05 15:30:25 utils.py:132] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 05-05 15:30:42 model_runner.py:175] Loading model weights took 7.4829 GB
[36m(RayWorkerWrapper pid=26415)[0m INFO 05-05 15:30:43 model_runner.py:175] Loading model weights took 7.4829 GB
INFO 05-05 15:30:44 distributed_gpu_executor.py:45] # GPU blocks: 61151, # CPU blocks: 4096
[36m(RayWorkerWrapper pid=26415)[0m INFO 05-05 15:30:45 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=26415)[0m INFO 05-05 15:30:45 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-05 15:30:45 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 05-05 15:30:45 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-05 15:30:51 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
INFO 05-05 15:30:51 model_runner.py:1017] Graph capturing finished in 6 secs.
[36m(RayWorkerWrapper pid=26415)[0m INFO 05-05 15:30:51 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
[36m(RayWorkerWrapper pid=26415)[0m INFO 05-05 15:30:51 model_runner.py:1017] Graph capturing finished in 6 secs.
Reading samples...
Running test suites...
Writing results to results/codex_humaneval/{self-seq-self-seq-Meta-Llama-3-8B-flancot_llama_70b}_temp_0_8/codex_eval_predictions.jsonl_results.jsonl...
{'pass@10': 0.6941132048122534}
Loading model and tokenizer...
Average accuracy 0.210 - abstract_algebra
Average accuracy 0.511 - anatomy
Average accuracy 0.487 - astronomy
Average accuracy 0.340 - business_ethics
Average accuracy 0.585 - clinical_knowledge
Average accuracy 0.500 - college_biology
Average accuracy 0.410 - college_chemistry
Average accuracy 0.360 - college_computer_science
Average accuracy 0.270 - college_mathematics
Average accuracy 0.376 - college_medicine
Average accuracy 0.412 - college_physics
Average accuracy 0.290 - computer_security
Average accuracy 0.489 - conceptual_physics
Average accuracy 0.281 - econometrics
Average accuracy 0.386 - electrical_engineering
Average accuracy 0.307 - elementary_mathematics
Average accuracy 0.413 - formal_logic
Average accuracy 0.200 - global_facts
Average accuracy 0.565 - high_school_biology
Average accuracy 0.305 - high_school_chemistry
Average accuracy 0.210 - high_school_computer_science
Average accuracy 0.345 - high_school_european_history
Average accuracy 0.631 - high_school_geography
Average accuracy 0.653 - high_school_government_and_politics
Average accuracy 0.462 - high_school_macroeconomics
Average accuracy 0.285 - high_school_mathematics
Average accuracy 0.483 - high_school_microeconomics
Average accuracy 0.252 - high_school_physics
Average accuracy 0.538 - high_school_psychology
Average accuracy 0.407 - high_school_statistics
Average accuracy 0.377 - high_school_us_history
Average accuracy 0.409 - high_school_world_history
Average accuracy 0.619 - human_aging
Average accuracy 0.473 - human_sexuality
Average accuracy 0.405 - international_law
Average accuracy 0.426 - jurisprudence
Average accuracy 0.417 - logical_fallacies
Average accuracy 0.375 - machine_learning
Average accuracy 0.680 - management
Average accuracy 0.551 - marketing
Average accuracy 0.600 - medical_genetics
Average accuracy 0.483 - miscellaneous
Average accuracy 0.364 - moral_disputes
Average accuracy 0.353 - moral_scenarios
Average accuracy 0.552 - nutrition
Average accuracy 0.373 - philosophy
Average accuracy 0.448 - prehistory
Average accuracy 0.312 - professional_accounting
Average accuracy 0.287 - professional_law
Average accuracy 0.537 - professional_medicine
Average accuracy 0.389 - professional_psychology
Average accuracy 0.482 - public_relations
Average accuracy 0.420 - security_studies
Average accuracy 0.448 - sociology
Average accuracy 0.670 - us_foreign_policy
Average accuracy 0.422 - virology
Average accuracy 0.749 - world_religions
Average accuracy 0.309 - math
Average accuracy 0.532 - health
Average accuracy 0.420 - physics
Average accuracy 0.533 - business
Average accuracy 0.544 - biology
Average accuracy 0.340 - chemistry
Average accuracy 0.311 - computer science
Average accuracy 0.441 - economics
Average accuracy 0.386 - engineering
Average accuracy 0.401 - philosophy
Average accuracy 0.417 - other
Average accuracy 0.404 - history
Average accuracy 0.631 - geography
Average accuracy 0.539 - politics
Average accuracy 0.459 - psychology
Average accuracy 0.458 - culture
Average accuracy 0.304 - law
Average accuracy 0.375 - STEM
Average accuracy 0.365 - humanities
Average accuracy 0.482 - social sciences
Average accuracy 0.491 - other (business, health, misc.)
Average accuracy: 0.422
