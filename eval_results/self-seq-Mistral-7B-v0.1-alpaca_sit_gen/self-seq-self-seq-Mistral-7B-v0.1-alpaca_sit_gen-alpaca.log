Loading data...
Loading model and tokenizer...
INFO 05-20 07:48:30 llm_engine.py:100] Initializing an LLM engine (v0.4.1) with config: model='output/self-seq-Mistral-7B-v0.1-alpaca_sit_gen', speculative_config=None, tokenizer='output/self-seq-Mistral-7B-v0.1-alpaca_sit_gen', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 05-20 07:48:34 utils.py:620] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=28166)[0m INFO 05-20 07:48:34 utils.py:620] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 05-20 07:48:34 selector.py:28] Using FlashAttention-2 backend.
[36m(RayWorkerWrapper pid=28166)[0m INFO 05-20 07:48:35 selector.py:28] Using FlashAttention-2 backend.
INFO 05-20 07:48:37 pynccl_utils.py:43] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=28166)[0m INFO 05-20 07:48:37 pynccl_utils.py:43] vLLM is using nccl==2.18.1
INFO 05-20 07:48:37 utils.py:129] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=28166)[0m INFO 05-20 07:48:37 utils.py:129] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 05-20 07:48:57 model_runner.py:172] Loading model weights took 6.7544 GB
[36m(RayWorkerWrapper pid=28166)[0m INFO 05-20 07:48:58 model_runner.py:172] Loading model weights took 6.7544 GB
INFO 05-20 07:49:00 distributed_gpu_executor.py:45] # GPU blocks: 60585, # CPU blocks: 4096
INFO 05-20 07:49:01 model_runner.py:872] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 05-20 07:49:01 model_runner.py:876] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=28166)[0m INFO 05-20 07:49:01 model_runner.py:872] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=28166)[0m INFO 05-20 07:49:01 model_runner.py:876] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-20 07:49:11 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
INFO 05-20 07:49:11 model_runner.py:953] Graph capturing finished in 10 secs.
[36m(RayWorkerWrapper pid=28166)[0m INFO 05-20 07:49:11 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
[36m(RayWorkerWrapper pid=28166)[0m INFO 05-20 07:49:11 model_runner.py:953] Graph capturing finished in 10 secs.
Calculating accuracy...
Exact match : 0.014404852160727824
Loading data...
Loading model and tokenizer...
INFO 05-20 07:53:34 llm_engine.py:100] Initializing an LLM engine (v0.4.1) with config: model='output/self-seq-Mistral-7B-v0.1-alpaca_sit_gen', speculative_config=None, tokenizer='output/self-seq-Mistral-7B-v0.1-alpaca_sit_gen', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 05-20 07:53:38 utils.py:620] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=30288)[0m INFO 05-20 07:53:38 utils.py:620] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 05-20 07:53:38 selector.py:28] Using FlashAttention-2 backend.
[36m(RayWorkerWrapper pid=30288)[0m INFO 05-20 07:53:39 selector.py:28] Using FlashAttention-2 backend.
INFO 05-20 07:53:40 pynccl_utils.py:43] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=30288)[0m INFO 05-20 07:53:40 pynccl_utils.py:43] vLLM is using nccl==2.18.1
INFO 05-20 07:53:41 utils.py:129] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=30288)[0m INFO 05-20 07:53:41 utils.py:129] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=30288)[0m INFO 05-20 07:54:01 model_runner.py:172] Loading model weights took 6.7544 GB
INFO 05-20 07:54:01 model_runner.py:172] Loading model weights took 6.7544 GB
INFO 05-20 07:54:03 distributed_gpu_executor.py:45] # GPU blocks: 60585, # CPU blocks: 4096
INFO 05-20 07:54:04 model_runner.py:872] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 05-20 07:54:04 model_runner.py:876] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=30288)[0m INFO 05-20 07:54:04 model_runner.py:872] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=30288)[0m INFO 05-20 07:54:04 model_runner.py:876] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-20 07:54:14 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
INFO 05-20 07:54:14 model_runner.py:953] Graph capturing finished in 10 secs.
[36m(RayWorkerWrapper pid=30288)[0m INFO 05-20 07:54:14 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
[36m(RayWorkerWrapper pid=30288)[0m INFO 05-20 07:54:14 model_runner.py:953] Graph capturing finished in 10 secs.
Calculating accuracy...
Exact match for en: 0.008
Calculating accuracy...
Exact match for es: 0.02
Calculating accuracy...
Exact match for fr: 0.008
Calculating accuracy...
Exact match for de: 0.024
Calculating accuracy...
Exact match for ru: 0.004
Calculating accuracy...
Exact match for zh: 0.012
Calculating accuracy...
Exact match for ja: 0.0
Calculating accuracy...
Exact match for th: 0.02
Calculating accuracy...
Exact match for sw: 0.02
Calculating accuracy...
Exact match for bn: 0.02
Calculating accuracy...
Exact match for te: 0.02
Loading data...
Loading model and tokenizer...
INFO 05-20 08:19:24 llm_engine.py:100] Initializing an LLM engine (v0.4.1) with config: model='output/self-seq-Mistral-7B-v0.1-alpaca_sit_gen', speculative_config=None, tokenizer='output/self-seq-Mistral-7B-v0.1-alpaca_sit_gen', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 05-20 08:19:29 utils.py:620] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=32440)[0m INFO 05-20 08:19:29 utils.py:620] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 05-20 08:19:29 selector.py:28] Using FlashAttention-2 backend.
[36m(RayWorkerWrapper pid=32440)[0m INFO 05-20 08:19:30 selector.py:28] Using FlashAttention-2 backend.
INFO 05-20 08:19:31 pynccl_utils.py:43] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=32440)[0m INFO 05-20 08:19:31 pynccl_utils.py:43] vLLM is using nccl==2.18.1
INFO 05-20 08:19:32 utils.py:129] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=32440)[0m INFO 05-20 08:19:32 utils.py:129] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 05-20 08:19:52 model_runner.py:172] Loading model weights took 6.7544 GB
[36m(RayWorkerWrapper pid=32440)[0m INFO 05-20 08:19:52 model_runner.py:172] Loading model weights took 6.7544 GB
INFO 05-20 08:19:54 distributed_gpu_executor.py:45] # GPU blocks: 60585, # CPU blocks: 4096
INFO 05-20 08:19:55 model_runner.py:872] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 05-20 08:19:55 model_runner.py:876] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=32440)[0m INFO 05-20 08:19:55 model_runner.py:872] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=32440)[0m INFO 05-20 08:19:55 model_runner.py:876] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-20 08:20:05 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
INFO 05-20 08:20:05 model_runner.py:953] Graph capturing finished in 10 secs.
[36m(RayWorkerWrapper pid=32440)[0m INFO 05-20 08:20:05 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
[36m(RayWorkerWrapper pid=32440)[0m INFO 05-20 08:20:05 model_runner.py:953] Graph capturing finished in 10 secs.
Calculating accuracy...
Exact match for en: 0.004
Calculating accuracy...
Exact match for es: 0.004
Calculating accuracy...
Exact match for fr: 0.02
Calculating accuracy...
Exact match for de: 0.02
Calculating accuracy...
Exact match for ru: 0.008
Calculating accuracy...
Exact match for zh: 0.028
Calculating accuracy...
Exact match for ja: 0.004
Calculating accuracy...
Exact match for th: 0.016
Calculating accuracy...
Exact match for sw: 0.048
Calculating accuracy...
Exact match for bn: 0.016
Calculating accuracy...
Exact match for te: 0.016
Number of examples: 164
INFO 05-20 08:48:15 llm_engine.py:100] Initializing an LLM engine (v0.4.1) with config: model='output/self-seq-Mistral-7B-v0.1-alpaca_sit_gen', speculative_config=None, tokenizer='output/self-seq-Mistral-7B-v0.1-alpaca_sit_gen', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)
INFO 05-20 08:48:19 utils.py:620] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
[36m(RayWorkerWrapper pid=34452)[0m INFO 05-20 08:48:19 utils.py:620] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 05-20 08:48:20 selector.py:28] Using FlashAttention-2 backend.
[36m(RayWorkerWrapper pid=34452)[0m INFO 05-20 08:48:20 selector.py:28] Using FlashAttention-2 backend.
INFO 05-20 08:48:21 pynccl_utils.py:43] vLLM is using nccl==2.18.1
[36m(RayWorkerWrapper pid=34452)[0m INFO 05-20 08:48:21 pynccl_utils.py:43] vLLM is using nccl==2.18.1
INFO 05-20 08:48:21 utils.py:129] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
[36m(RayWorkerWrapper pid=34452)[0m INFO 05-20 08:48:21 utils.py:129] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json
INFO 05-20 08:48:41 model_runner.py:172] Loading model weights took 6.7544 GB
[36m(RayWorkerWrapper pid=34452)[0m INFO 05-20 08:48:41 model_runner.py:172] Loading model weights took 6.7544 GB
INFO 05-20 08:48:43 distributed_gpu_executor.py:45] # GPU blocks: 60585, # CPU blocks: 4096
INFO 05-20 08:48:44 model_runner.py:872] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 05-20 08:48:44 model_runner.py:876] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(RayWorkerWrapper pid=34452)[0m INFO 05-20 08:48:44 model_runner.py:872] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerWrapper pid=34452)[0m INFO 05-20 08:48:44 model_runner.py:876] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-20 08:48:49 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
INFO 05-20 08:48:49 model_runner.py:953] Graph capturing finished in 5 secs.
[36m(RayWorkerWrapper pid=34452)[0m INFO 05-20 08:48:49 custom_all_reduce.py:246] Registering 2275 cuda graph addresses
[36m(RayWorkerWrapper pid=34452)[0m INFO 05-20 08:48:49 model_runner.py:953] Graph capturing finished in 5 secs.
Reading samples...
Running test suites...
Writing results to results/codex_humaneval/{self-seq-self-seq-Mistral-7B-v0.1-alpaca_sit_gen}_temp_0_8/codex_eval_predictions.jsonl_results.jsonl...
{'pass@10': 0.13450607762697167}
Loading model and tokenizer...
Average accuracy 0.360 - abstract_algebra
Average accuracy 0.519 - anatomy
Average accuracy 0.428 - astronomy
Average accuracy 0.410 - business_ethics
Average accuracy 0.438 - clinical_knowledge
Average accuracy 0.458 - college_biology
Average accuracy 0.230 - college_chemistry
Average accuracy 0.310 - college_computer_science
Average accuracy 0.280 - college_mathematics
Average accuracy 0.393 - college_medicine
Average accuracy 0.255 - college_physics
Average accuracy 0.600 - computer_security
Average accuracy 0.336 - conceptual_physics
Average accuracy 0.298 - econometrics
Average accuracy 0.366 - electrical_engineering
Average accuracy 0.286 - elementary_mathematics
Average accuracy 0.190 - formal_logic
Average accuracy 0.310 - global_facts
Average accuracy 0.448 - high_school_biology
Average accuracy 0.350 - high_school_chemistry
Average accuracy 0.410 - high_school_computer_science
Average accuracy 0.491 - high_school_european_history
Average accuracy 0.439 - high_school_geography
Average accuracy 0.487 - high_school_government_and_politics
Average accuracy 0.377 - high_school_macroeconomics
Average accuracy 0.267 - high_school_mathematics
Average accuracy 0.357 - high_school_microeconomics
Average accuracy 0.298 - high_school_physics
Average accuracy 0.457 - high_school_psychology
Average accuracy 0.273 - high_school_statistics
Average accuracy 0.500 - high_school_us_history
Average accuracy 0.498 - high_school_world_history
Average accuracy 0.426 - human_aging
Average accuracy 0.443 - human_sexuality
Average accuracy 0.628 - international_law
Average accuracy 0.528 - jurisprudence
Average accuracy 0.399 - logical_fallacies
Average accuracy 0.321 - machine_learning
Average accuracy 0.476 - management
Average accuracy 0.641 - marketing
Average accuracy 0.400 - medical_genetics
Average accuracy 0.596 - miscellaneous
Average accuracy 0.451 - moral_disputes
Average accuracy 0.247 - moral_scenarios
Average accuracy 0.422 - nutrition
Average accuracy 0.469 - philosophy
Average accuracy 0.491 - prehistory
Average accuracy 0.351 - professional_accounting
Average accuracy 0.332 - professional_law
Average accuracy 0.279 - professional_medicine
Average accuracy 0.423 - professional_psychology
Average accuracy 0.464 - public_relations
Average accuracy 0.331 - security_studies
Average accuracy 0.527 - sociology
Average accuracy 0.550 - us_foreign_policy
Average accuracy 0.349 - virology
Average accuracy 0.661 - world_religions
Average accuracy 0.285 - math
Average accuracy 0.398 - health
Average accuracy 0.336 - physics
Average accuracy 0.549 - business
Average accuracy 0.452 - biology
Average accuracy 0.310 - chemistry
Average accuracy 0.408 - computer science
Average accuracy 0.358 - economics
Average accuracy 0.366 - engineering
Average accuracy 0.360 - philosophy
Average accuracy 0.512 - other
Average accuracy 0.495 - history
Average accuracy 0.439 - geography
Average accuracy 0.434 - politics
Average accuracy 0.439 - psychology
Average accuracy 0.494 - culture
Average accuracy 0.364 - law
Average accuracy 0.344 - STEM
Average accuracy 0.388 - humanities
Average accuracy 0.424 - social sciences
Average accuracy 0.459 - other (business, health, misc.)
Average accuracy: 0.403
